{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boriskuchin/MADMO-BASE-2024/blob/main/12_decision_trees_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решающие деревья. Случайный лес"
      ],
      "metadata": {
        "id": "BRzZ0mkJfdnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Введение"
      ],
      "metadata": {
        "id": "k0ewjzenfhUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Простое определение"
      ],
      "metadata": {
        "id": "VgIOQw71fljf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Дерево принятия решений (decision tree)** - алгоритм машинного обучения, предсказывает целевую переменную с помощью применения последовательности простых решающих правил (предикатов).\n"
      ],
      "metadata": {
        "id": "R9mFKL5TfoAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Наглядный пример"
      ],
      "metadata": {
        "id": "RgAzq7EMftgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.kym-cdn.com/entries/icons/original/000/006/117/81939832_3194411770585244_1699876574616092672_n.png)"
      ],
      "metadata": {
        "id": "tJg-w00Cfx1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Менее наглядный пример"
      ],
      "metadata": {
        "id": "6x8sS3ndftbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://yastatic.net/s3/ml-handbook/admin/3_2_41c1793bef.png)"
      ],
      "metadata": {
        "id": "5blgani7f9cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Более строгое определение"
      ],
      "metadata": {
        "id": "ddXH28tZgIhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Решающее дерево** - бинарное дерево, в котором:\n",
        "1. каждой внутренней вершине $v$ приписан предикат $B_v: \\mathbb{X} \\to \\{0,1\\}$\n",
        "2. каждой листовой вершине $v$ приписан прогноз $c_v \\in \\mathbb{Y}$, где $\\mathbb{Y}$ — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов)."
      ],
      "metadata": {
        "id": "m8a4e4CGgLGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Предсказание** - проход по этому дереву от корня к листу такой, что в каждой встречаемой внутренней вершине $v$ считаем значение $B_v(x)$: если $B_v(x)=1$, то идем вправо, если $B_v(x)=0$, то идем влево."
      ],
      "metadata": {
        "id": "IWxiGU0XgzQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Структура предиката"
      ],
      "metadata": {
        "id": "BCMCtdRfhYSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вообще предикат $B_v(x)$ может иметь любую структуру, однако, на практике чаще всего используют сравнение j-го признака с некоторым пороговым значением:"
      ],
      "metadata": {
        "id": "_CJH2VLPhXV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$B_v(x, j, t) = [x_j \\leq t]$$"
      ],
      "metadata": {
        "id": "N8eN7jYnhrat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из структуры предиката можем сделать следующие выводы:\n",
        "\n",
        "1. выученная функция является кусочно-постоянной, из-за чего производная равна нулю везде, где задана $\\to$ о градиентных методах при поиске оптимального решения можно забыть\n",
        "2. дерево решений не сможет экстраполировать зависимости за границы области значений обучающей выборки\n",
        "3. дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (переобучиться): дерево с одним объектом в каждом листе $\\to$ надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым."
      ],
      "metadata": {
        "id": "E4vPiS9Gh3iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Пример переобучения"
      ],
      "metadata": {
        "id": "E6VXN6BrjKKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переобученное дерево - не ограничивали глубину:"
      ],
      "metadata": {
        "id": "NRc3f4vCjR1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://yastatic.net/s3/ml-handbook/admin/3_5_74f1de3be9.png)"
      ],
      "metadata": {
        "id": "Rdc__vAgjMtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нормально обученное дерево - ограничили глубину значением 3:"
      ],
      "metadata": {
        "id": "CBcLlJijjW2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://yastatic.net/s3/ml-handbook/admin/3_4_aa20f33d21.png)"
      ],
      "metadata": {
        "id": "iqcbvkoojcU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение дерева"
      ],
      "metadata": {
        "id": "h55gO0hRjhwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение с учителем, имеем датасет $(X, y)$:\n",
        "- $X = \\{x_i^{(1)}, \\cdots, x_i^{(D)}\\}_{i=1}^N, \\quad x_i^{(j)} \\in \\mathbb{R} \\quad \\forall i, j $\n",
        "- $y = \\{y_i\\}_{i=1}^N, \\quad y_i \\in \\mathbb{R} \\quad \\forall i$\n",
        "\n",
        "Пусть так же задана функция потерь $L(f, X, y)$.\n",
        "\n",
        "Наша задача — построить решающее дерево $f$, наилучшим образом предсказывающее целевую зависимость - минимизируя $L$. Но градиенты использовать нельзя! Как быть?"
      ],
      "metadata": {
        "id": "-E3FJPFZjtFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решающие пни"
      ],
      "metadata": {
        "id": "0Jh9ee-km4xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте начнём с простого — научимся строить решающие пни, то есть решающие деревья глубины $1$:\n",
        "\n",
        "$$B_{j,t}(x) = [x_i^{(j)} \\leq t]$$"
      ],
      "metadata": {
        "id": "JrerQ1x9klbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эту задачу можно решить полным перебором: существует не более $(N-1)D$ предикатов такого вида - $j$ пробегает значения от $1$ до $D$, а всего значений порога $t$, при которых меняется значение предиката, может быть не более $N-1$:"
      ],
      "metadata": {
        "id": "-NJAXQr0lHBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://yastatic.net/s3/ml-handbook/admin/3_7_f51986c1ae.png)"
      ],
      "metadata": {
        "id": "lk69LRj3lrqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Искомое решение - $(j_{opt}, t_{opt}) = \\arg\\min\\limits_{j,t} L(B_{j,t}, X, y)$.\n",
        "\n",
        "Для каждого из $B_{j,t}$ нужно посчитать значение функции потерь - это еще $N$ операций. Итоговая сложность - $O(N^2D)$"
      ],
      "metadata": {
        "id": "UARMziwLl0OX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сложность неприятная, но не заоблачная. Проблема в том, что мы посчитали оптимум только для одной вершины, а что если их больше?"
      ],
      "metadata": {
        "id": "ae7OGeTTmQj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кроме того, при построении дерева нужно учесть склонность к переобучению, поэтому хотим построить **оптимальное по качеству** на обучающей выборке дерево **минимальной глубины**.\n",
        "\n",
        "Такая задача является $NP$-полной - проще говоря, кроме как полным перебором ее не решить :("
      ],
      "metadata": {
        "id": "GsDkPx5Jme54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как быть в такой ситуации? Есть два выхода:\n",
        "\n",
        "1. **Искать не оптимальное решение, а просто достаточно хорошее.**\n",
        "    \n",
        "    Строить дерево с помощью жадного алгоритма - не искать всю структуру сразу, а строить дерево \"этаж за этажом\". Тогда в каждой внутренней вершине дерева будет решаться задача, схожая с задачей построения решающего пня.\n",
        "    \n",
        "    Для того чтобы этот подход хоть как-то работал, его придётся прокачать внушительным набором эвристик.\n",
        "\n",
        "2. **Оптимизировать алгоритм полного перебора** — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз."
      ],
      "metadata": {
        "id": "uenK7nm-m65q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Жадный алгоритм построения решающего дерева"
      ],
      "metadata": {
        "id": "t6GfHfuEnbeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть $X$ — исходное множество объектов обучающей выборки, а $X_m$ — множество объектов, попавших в текущий лист (в самом начале), тогда жадный алгоритм условно можно описать следующим образом:\n",
        "\n",
        "1. Создаём вершину $v$\n",
        "2. Если выполнен **критерий остановки** $Stop(X_m)$\n",
        ", то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие **ответ** $Ans(X_m)$, после чего возвращаем её.\n",
        "    \n",
        "    Иначе: находим предикат (иногда ещё говорят сплит) $B_{j,t}$, который определит наилучшее разбиение текущего множества объектов $X_m$ на две подвыборки $X_l$ и $X_r$, максимизируя **критерий ветвления** $Branch(X_m, j, t)$.\n",
        "3. Для $X_l$ и $X_r$ рекурсивно повторим процедуру."
      ],
      "metadata": {
        "id": "EwpZnjG2nk1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный алгоритм содержит в себе вспомогательные функции, которые нужно выбрать так, чтобы итоговое дерево хороошо минимизировало $L$:\n",
        "\n",
        "- $Ans(X_m)$ - вычисляет ответ для листа по попавшим в него объектам из обучающей выборки:\n",
        "    - классификация - метка самого частого класса\n",
        "    - регрессия - среднее, медиана, другая статистика\n",
        "    - простая модель - можем здесь поместить другую модель!\n",
        "- Критерий остановки $Stop(X_m)$ - нужно ли продолжать ветвление:\n",
        "    - обычно - тривиальное правило: глубина, кол-во объектов в $X_m$, степень однородности объектов\n",
        "- Критерий ветвления $Branch(X_m, j, t)$ - измеряет, насколько хорошо предлагаемое разделение. Оценивает, насколько улучшится метрика качества по сравнению с не-ветвлением. С ее помощью их всех возможных сплитов выбираем тот, что дает наибольшее улучшение.\n",
        "\n",
        "Остановимся на критериях ветвление подробнее."
      ],
      "metadata": {
        "id": "LQ02N7vfbXXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Критерии ветвления"
      ],
      "metadata": {
        "id": "HAc9rxSyc7qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть $c$ - ответ дерева ($c \\in \\mathbb{R}$ для регрессии, $c \\in \\mathbb{N}$ или $c \\in \\mathbb{R}^K$ при $\\sum_{i=1}^K c_i = 1$ для классификации), а $L(y_i, c)$ - заданная функция потерь."
      ],
      "metadata": {
        "id": "2EwmwaIZc-AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ищем оптимальный сплит $X_m  = X_l \\cup X_r$. Если бы текущая вершина была листом, то ее ответ был бы $c$, тогда значение функции потерь:\n",
        "\n",
        "$$\\frac{1}{|X_m|} \\sum_{(x_i,y_i) \\in X_m} L(y_i, c)$$\n",
        "\n",
        "Оптимальное значение этой величины:\n",
        "\n",
        "$$H(X_m) = \\min\\limits_{c \\in Y} \\frac{1}{|X_m|}\\sum_{(x_i,y_i) \\in X_m} L(y_i, c)$$\n",
        "\n",
        "называют **информативностью** (**impurity**). Чем она ниже, тем лучше объекты в листе можно приблизить константой."
      ],
      "metadata": {
        "id": "NlzjxM6OdkRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для решающего пня получим:\n",
        "\n",
        "$$\\frac{1}{|X_m|} \\big( \\sum_{(x_i,y_i) \\in X_l} L(y_i, c_l) + \\sum_{(x_i,y_i) \\in X_r} L(y_i, c_r) \\big)$$\n",
        "\n",
        "Тогда разность информативностей исходной вершины и решающего пня:\n",
        "\n",
        "$$H(X_m) - \\frac{|X_l|}{|X_m|} H(X_l) - \\frac{|X_r|}{|X_m|} H(X_r)$$\n",
        "\n",
        "Для симметрии можем домножить на $|X_m|$:\n",
        "\n",
        "$$Branch(X_m, j, t) = |X_m| \\cdot H(X_m) - |X_l| \\cdot H(X_l) - |X_r| \\cdot H(X_r)$$"
      ],
      "metadata": {
        "id": "d3KYz0rvez0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта величина неотрицательна - разделив объекты, хуже не сделаем. Чем она больше, тем лучше предлагаемое разделение."
      ],
      "metadata": {
        "id": "NNtsLwS8fp87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь рассмотрим, какие будут критерии ветвления для типичных задач:"
      ],
      "metadata": {
        "id": "eXMuJwYuf51Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Регрессия - MSE"
      ],
      "metadata": {
        "id": "FK3lQoCmgCWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При MSE - $L(y_i,c ) = (y_i - c)^2$, тогда информативность:\n",
        "\n",
        "$$H(X_m) = \\min\\limits_{c \\in Y} \\frac{1}{|X_m|}\\sum_{(x_i,y_i) \\in X_m} (y_i - c)^2$$"
      ],
      "metadata": {
        "id": "y5nSDQpIgEou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимальное предсказание константного классификатора для MSE - среднее арифметическое $c = \\frac{\\sum y_i}{|X_m|}$:\n",
        "\n",
        "$$H(X_m) = \\sum_{(x_i,y_i) \\in X_m} \\frac{(y_i - \\bar{y})^2}{|X_m|}$$\n",
        "\n",
        "При жадной минимизации MSE информативность - оценка дисперсии целевых величин для объектов, попавших в лист.\n",
        "\n",
        "Тогда выбирать сплиты нужно так, чтобы сумма дисперсий в листьях была как можно меньше."
      ],
      "metadata": {
        "id": "ekPuPCozgVwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Регрессия - MAE"
      ],
      "metadata": {
        "id": "2EAMUfX7hFQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При MAE - $L(y_i,c ) = |y_i - c|$, оптимальное предсказание - медиана:\n",
        "\n",
        "$$H(X_m) = \\sum_{(x_i,y_i) \\in X_m} \\frac{|y_i - MEDIAN(y)|}{|X_m|}$$"
      ],
      "metadata": {
        "id": "s1vv2XYkhP47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Классификация - Misclassification Error"
      ],
      "metadata": {
        "id": "Qhtn66idhxKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть в задаче $K$ классов, $p_k$ - доля объектов класса $k$ в текущей вершине:\n",
        "\n",
        "$$p_k = \\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} \\mathbb{I}[y_i = k]$$"
      ],
      "metadata": {
        "id": "qHD7_zauh00a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам интересна доля верно угаданных классов, функция потерь - $L(y_i, c) = \\mathbb{I}[y_i \\neq c]$.\n",
        "\n",
        "Пусть предсказание модели в листе - один класс, тогда информативность:\n",
        "\n",
        "$$H(X_m) = \\min\\limits_{c \\in Y} \\frac{1}{|X_m|}\\sum_{(x_i,y_i) \\in X_m} \\mathbb{I}[y_i \\neq c]$$"
      ],
      "metadata": {
        "id": "PRIlCeuFiILh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимальное предсказание - самый частый класс $k_*$, информативность получится:\n",
        "\n",
        "$$H(X_m) = \\frac{1}{|X_m|}\\sum_{(x_i,y_i) \\in X_m} \\mathbb{I}[y_i \\neq k_*] = 1 - p_{k_*} = 1 - \\max\\limits_{k \\in K} p_k$$"
      ],
      "metadata": {
        "id": "N-2rtlEOidKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Классификация - Энтропия"
      ],
      "metadata": {
        "id": "XJIy7zJeixLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предсказываем вероятностное распределение классов $(c_1, \\cdots, c_K)$, можем подойти как к задаче логистической регрессии - через максимизацию логарифма правдоподобия. Пусть в вершине предсказывается фиксированное распределение $c$ (независящее от $x_i$), тогда правдоподобие:\n",
        "\n",
        "$$ P(y|x, c) = P(y|c) = \\prod_{(x_i,y_i) \\in X_m} P(y_i|c) = \\prod_{(x_i,y_i) \\in X_m} \\prod_{k=1}^K c_k^{\\mathbb{I}[y_i=k]}, $$\n",
        "\n",
        "прологарифмируем и получим:\n",
        "\n",
        "$$ H(X_m) = \\min\\limits_{\\sum_k c_k = 1} \\big( -\\frac{1}{|X_m|} \\sum_{(x_i,y_i) \\in X_m} \\sum_{k=1}^K \\mathbb{I}[y_i=k] \\log c_k \\big) $$"
      ],
      "metadata": {
        "id": "wj2A-Seei2Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимальная оценка вероятностей $c_k$ - это $p_k$, т.е. доля попавших в лист объектов этого класса - выводится через условный минимум (лагранжиан).\n",
        "\n",
        "\n",
        "Подставим вектор $c=(p_1, \\cdots, p_K)$:\n",
        "\n",
        "$$H(X_m) = - \\sum_{k=1}^K p_k \\log p_k$$"
      ],
      "metadata": {
        "id": "lSan8JjRkJEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Классификация - Критерий Джини"
      ],
      "metadata": {
        "id": "Bh8-quOwkma5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Внимание!**\n",
        ">\n",
        "> Не путать с индексом Джини из экономики! Это разные вещи!"
      ],
      "metadata": {
        "id": "tWgnJ6WyoOWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предсказание - распределение вероятности $(c_1, \\cdots, c_K)$. Вместо логарифма правдоподобия в качестве критерия можно использовать метрику (по сути - MSE по вероятностям).\n",
        "\n",
        "Тогда информативность:\n",
        "\n",
        "$$ H(X_m) = \\min\\limits_{\\sum_k c_k = 1} \\frac{1}{|X_m|} \\sum_{(x_i,y_i) \\in X_m} \\sum_{k=1}^K (c_k - \\mathbb{I}[y_i=k])^2 $$"
      ],
      "metadata": {
        "id": "_l5iH8yPm_tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимальное занчение этой метрики - выборочные оценки частот классов $(p_1, \\cdots, p_K)$, $p_i = \\frac{1}{|X_m|} \\sum_{i} \\mathbb{I}[y_i = k]$. Подставим в выражение выше, упростим и получим:\n",
        "\n",
        "$$H(X_m) = \\sum_{k=1}^K p_k(1-p_k) = 1 - \\sum_{k=1}^K p_k^2 $$"
      ],
      "metadata": {
        "id": "SZZlRkKFnms6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сравнение"
      ],
      "metadata": {
        "id": "pu0qNCSsos2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://habrastorage.org/r/w1560/files/a88/bc3/e18/a88bc3e185b246e088a4382e212e4473.png)"
      ],
      "metadata": {
        "id": "ODyVXxLBoueJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Неоптимальность критериев"
      ],
      "metadata": {
        "id": "kBNzqGj_o7t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Несмотря на все наши телодвиждения выше, жадный алгоритм не всегда позволяет найти самое оптимальное решение.\n",
        "\n",
        "Простейший пример - задача XOR:"
      ],
      "metadata": {
        "id": "CCbl47cwo_N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://yastatic.net/s3/ml-handbook/admin/3_10_08f8ee6402.png)"
      ],
      "metadata": {
        "id": "G-6WGkMbpKWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вне зависимости от того, что вы оптимизируете, жадный алгоритм не даст оптимального решения задачи XOR.\n",
        "\n",
        "Но этим примером проблемы не исчерпываются. Бывают ситуации, когда оптимальное с точки зрения выбранной метрики дерево вы получите с критерием ветвления, построенным по другой метрике (например, MSE-критерий для MAE-задачи или Джини для misclassification error)."
      ],
      "metadata": {
        "id": "HU2OckdXpP22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Особенности работы с данными"
      ],
      "metadata": {
        "id": "cZSquIeIpZPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Категориальные признаки"
      ],
      "metadata": {
        "id": "qRlVXhX-pcSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На первый взгляд - все отлично, $x^{(i)} \\in C$, при разбиении будет $C_m = C_l \\cup C_r$, предикат вида $[x^{(i)} \\in C_r]$. Это очень логично и естественно, но количество сплитов - $2^{M-1} -1$, перебирать очень долго.\n",
        "\n",
        "\n",
        "Намного лучше упорядочить значения $c_m$ и работать как с числами."
      ],
      "metadata": {
        "id": "c7ozgdoapfMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Работа с пропусками"
      ],
      "metadata": {
        "id": "YxHrpaUuqUdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Одна из приятных особенностей деревьев — способность обрабатывать пропуски в данных. Разберёмся, что при этом происходит на этапе обучения и на этапе применения дерева."
      ],
      "metadata": {
        "id": "BTJwHtAyqXiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть у нас есть некоторый признак $x^{(i)}$, значение которого пропущено у некоторых объектов, $X_m$ - множество объектов, пришедших в рассматриваемую вершину, $V_m$ — подмножество $X_m$, состоящее из объектов с пропущенным значением $x^{(i)}$.\n",
        "\n",
        "В момент выбора сплитов по этому признаку мы будем просто игнорировать объекты из $V_m$, а когда сплит выбран, мы отправим их в оба поддерева, присвоив им веса $\\frac{|X_l|}{|X_m|}$ для левого и $\\frac{|X_r|}{|X_m|}$ для правого поддеревьев."
      ],
      "metadata": {
        "id": "ngpY846Aqbfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этапе предсказания, если в вершину, в которой сплит идет по пропущенному признаку, пришел объект с пропущенным значением в этом признаке, то отправляем его в каждую из веток, получаем предсказания, взвешиваем с весами $\\frac{|X_l|}{|X_m|}$ для левого и $\\frac{|X_r|}{|X_m|}$ для правого поддеревьев."
      ],
      "metadata": {
        "id": "z_5GWyarq47c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Регуляризация деревьев"
      ],
      "metadata": {
        "id": "Iu4Jr2qOrOL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Деревья легко переобучаются, процесс ветвления нужно ограничивать."
      ],
      "metadata": {
        "id": "ltDiZ8yhrVP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные способы:\n",
        "\n",
        "- ограничение по максимальной глубине дерева;\n",
        "- ограничение на минимальное количество объектов в листе;\n",
        "- ограничение на максимальное количество листьев в дереве;\n",
        "- требование, чтобы функционал качества $Branch$ при делении текущей подвыборки на две улучшался не менее чем на $s$ процентов."
      ],
      "metadata": {
        "id": "gr6PtL_draXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делать это можно на разных этапах работы алгоритма:\n",
        "\n",
        "- во время построения дерева - **pre-pruning** или **early stopping**\n",
        "- построить дерево жадно без ограничений, а затем провести стрижку (**pruning**) - удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке."
      ],
      "metadata": {
        "id": "4gYmWbsKriLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ансамбли"
      ],
      "metadata": {
        "id": "jg3ZEokosD_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ансамблевые методы** - парадигма машинного обучения, в которой несколько моделей обучаются для решения одной и той же проблемы и объединяются для получения лучших результатов. Основная гипотеза - при правильном сочетании слабых моделей мы можем получить более точные и/или надежные модели.\n",
        "\n",
        "Вводится понятие **_слабых учеников_** (или **_базовых моделей_**). Часто базовые модели работают сами по себе не так хорошо в связи с тем, для этого есть две причины:\n",
        "- имеют высокое смещение - например, модели с низкой степенью свободы\n",
        "- имеют слишком большой разброс, чтобы быть устойчивыми (например, модели с высокой степенью свободы).\n",
        "\n",
        "**Идея ансамблевых методов** - попытаться уменьшить смещение и/или разброс таких слабых учеников, объединяя несколько из них вместе, чтобы создать **_сильного ученика_** (или **_модель ансамбля_**), который достигает лучших результатов."
      ],
      "metadata": {
        "id": "5h1zrwymDOpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Математическое обоснование"
      ],
      "metadata": {
        "id": "MeVFapl8DSDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть решаем задачу регрессии и обучаем $n$ моделей, каждая из которых имеет ошибку $\\varepsilon_i$. Будем считать, что все ошибки распределены по нормальному закону с нулевым средним $\\mathbb{E}[\\varepsilon_i]=0$, дисперсией $\\mathbb{E}[\\varepsilon_i^2]=\\nu$ и ковариацией $\\mathbb{E}[\\varepsilon_i \\varepsilon_j]=c$. В таком случае средняя ошибка предсказания ансамбля будет равна:\n",
        "\n",
        "$$ \\mathbb{E}[(\\frac{1}{n}\\sum_i\\varepsilon_i)^2] = \\frac{1}{n^2} \\mathbb{E}[\\sum_i (\\varepsilon_i^2 + \\sum_{j \\ne i}\\varepsilon_i \\varepsilon_j)] = \\frac{1}{n} \\nu + \\frac{n-1}{n} c $$"
      ],
      "metadata": {
        "id": "tTBFhpFdDT3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим два крайных случая:\n",
        "- $c = \\nu$ - ошибки разных моделей идеально скоррелированы, получаем, что квадрат ошибки никак не изменился\n",
        "- $c = 0$ - ошибки разных моделей полностью независимы, получаем линейное уменьшение ошибки с ростом количества моделей в ансамбле"
      ],
      "metadata": {
        "id": "2eeMhc3qDWiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отсюда можно сделать следующие выводы:\n",
        "1. ансамблирование моделей с одинаковыми ошибками не уменьшает ошибку ансамбля\n",
        "2. чтобы получить значительное уменьшение ошибки мы должны ансамблировать модели, в которых предсказания, а следовательно и ошибки, сильно отличаются\n",
        "\n",
        "Для получения моделей с нескоррелированными предсказаниями, используются следующие подходы:\n",
        "- обучить модели на разных поднаборах данных\n",
        "- обучить модели на разных поднаборах признаках\n",
        "- обучить модели с разной начальной инициализацией параметров\n",
        "- обучить разные типы моделей модели"
      ],
      "metadata": {
        "id": "Nm16XdtMDX3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Небольшое забегание вперед:\n",
        "- **Bootstrap** - статистический метод, заключающийся в генерации выборок размера $B$ (так называемых бутстрэп-выборок) из исходного датасета размера $L$ путем случайного выбора элементов с повторениями в каждом из наблюдений $B$.\n",
        "- **Бэггинг** (**bagging**, **bootstrap aggregation**) - один из основных методов ансамблирования. Основная идея - используем набор отдельных обучающих выборок $X^i$, полученных из исходной выборки $X$ с помощью бутстрапа, для обучения набора базовых моделей $b_i(x)=b(x, X^i)$, а затем создаем из них ансамбль $a(x)$:\n",
        "$$a(x)=\\frac{1}{k} \\sum_{i=1}^k b_i(x)$$\n"
      ],
      "metadata": {
        "id": "95OXPwIIEQa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Случайный лес"
      ],
      "metadata": {
        "id": "HohyPp44sHjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Случайный лес** - ансамбль алгоритмов, где базовым алгоритмом является решающее дерево."
      ],
      "metadata": {
        "id": "HDCz76B3Dhxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Алгоритм построения"
      ],
      "metadata": {
        "id": "UST6uPdRDlkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Строим $k$ деревьев. Для построения $i$-го дерева:\n",
        "\t1. Сначала, как в обычном бэггинге, из обучающей выборки $X$ выбирается с возвращением случайная подвыборка $X_i$ того же размера, что и $X$.\n",
        "    2. В процессе обучения каждого дерева **в каждой вершине** случайно выбираются $n < N$ признаков, где $N$ — полное число признаков (**метод случайных подпространств, Random Subspace Method RSM**), и среди них ищется оптимальное разбиение. Такой приём как раз позволяет управлять степенью скоррелированности базовых алгоритмов.\n",
        "2. Чтобы получить предсказание ансамбля на тестовом объекте, усредняем отдельные ответы деревьев (для регрессии) или берём самый популярный класс (для классификации).\n",
        "3. Profit. Мы построили **Random Forest (случайный лес)** – комбинацию бэггинга и метода случайных подпространств над решающими деревьями."
      ],
      "metadata": {
        "id": "H1mNeuSRDnIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Гиперпараметры"
      ],
      "metadata": {
        "id": "TTfz7fUrDuyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Глубина деревьев"
      ],
      "metadata": {
        "id": "n0sWwSt5DxTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Т.к. бэггинг позволяет снизить разброс, но не смещение, то стоит отдавать предпочтения алгоритмам с низким смещением. Такими алгоритмами являются глубокие деревья, которые хорошо переобучаются и могут запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко к истине (высокая дисперсия, низкое смещение).\n",
        "\n",
        "**Вывод:** используем глубокие деревья.\n"
      ],
      "metadata": {
        "id": "UYcZG8iGDzMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Количество признаков для отдельного дерева"
      ],
      "metadata": {
        "id": "M_xBOlBfD1wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ограничивая число признаков, которые используются в обучении одного дерева, мы также управляем качеством случайного леса:\n",
        "- Больше признаков - больше корреляция между деревьями, меньше чувствуется эффект от ансамблирования\n",
        "- Меньше признаков, тем слабее сами деревья.\n",
        "\n",
        "**Практическая рекомендация** – брать корень из числа всех признаков для классификации и треть признаков для регрессии."
      ],
      "metadata": {
        "id": "Zbn3D_eND3qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Количество деревьев в случайном лесе"
      ],
      "metadata": {
        "id": "sWKcbiVaD543"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Увеличение числа элементарных алгоритмов в ансамбле не меняет смещения и уменьшает разброс. Т.к. число признаков и варианты подвыборок, на которых строятся деревья в случайном лесе, ограничены, уменьшать разброс до бесконечности не получится. Поэтому имеет смысл **построить график ошибки** от числа деревьев и ограничить размер леса в тот момент, когда ошибка перестанет значимо уменьшаться.\n",
        "\n",
        "Вторым практическим ограничением на количество деревьев может быть **время работы ансамбля**. Однако есть положительное свойство случайного леса: случайный лес можно строить и применять **параллельно**, что сокращает время работы, если у нас есть несколько процессоров. Но процессоров, скорее всего, всё же сильно меньше числа деревьев, а сами деревья обычно глубокие. Поэтому на большом числе деревьев Random Forest может работать дольше желаемого и количество деревьев можно сократить, немного пожертвовав качеством."
      ],
      "metadata": {
        "id": "EokaSMckD7xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Преимущества"
      ],
      "metadata": {
        "id": "yR62WppzD9tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Одна из лучших \"универсальных\" моделей - вместе с наивным баейсом, логистической регрессией\n",
        "2. Работает очень эффективно с пропущенными признаками - потому что на некоторых деревьях вообще не используется выпавших признак\n",
        "3. Есть классные модификации:\n",
        "\t- Extremely Randomized Trees - при разбиении признак вообще рандомно берем и для него ищем threshold\n",
        "\t- Isolation Forest - алгоритм поиска аномалий, можем поставить эту задачу деревьям\n",
        "4. Позволяет использовать тренировочный датасет для валидации - **Out Of Bag (OOB) estimation** (можно использовать и в обычном бэггинге) - обучение базовых моделей делается на подвыборке тестовой выборки, поэтому некоторые данные оттуда они не видели никогда."
      ],
      "metadata": {
        "id": "iDpnGM3MD_Nk"
      }
    }
  ]
}